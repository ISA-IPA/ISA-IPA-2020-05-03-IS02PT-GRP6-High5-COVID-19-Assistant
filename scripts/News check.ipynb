{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T08:13:40.961338Z",
     "start_time": "2020-04-12T08:13:40.958149Z"
    }
   },
   "outputs": [],
   "source": [
    "import tagui as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from goose3 import Goose\n",
    "from collections import Counter\n",
    "from math import fabs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T08:13:50.792191Z",
     "start_time": "2020-04-12T08:13:47.210330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "# t.close()\n",
    "t.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T08:13:58.381312Z",
     "start_time": "2020-04-12T08:13:58.375708Z"
    }
   },
   "outputs": [],
   "source": [
    "def wait_for_pageload(selector):\n",
    "    wait_status = 0\n",
    "    for loop_wait in range(1, 60):\n",
    "        print(f\"{loop_wait}. waiting for page to appear. wait for 1s...\")\n",
    "        if t.present(selector):\n",
    "            wait_status = 1\n",
    "            break\n",
    "        else:\n",
    "            t.wait(1)\n",
    "    print(\"Covid wait_status = {}\".format(wait_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T08:13:59.353077Z",
     "start_time": "2020-04-12T08:13:59.306263Z"
    }
   },
   "outputs": [],
   "source": [
    "stopWords = {\n",
    "    \"-\", \" \", \",\", \".\", \"a\", \"e\", \"i\", \"o\", \"u\", \"t\", \"about\", \"above\",\n",
    "    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n",
    "    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\",\n",
    "    \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\",\n",
    "    \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\",\n",
    "    \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\",\n",
    "    \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\",\n",
    "    \"between\", \"beyond\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\",\n",
    "    \"cannot\", \"can't\", \"co\", \"con\", \"could\", \"couldn't\", \"de\",\n",
    "    \"describe\", \"detail\", \"did\", \"do\", \"done\", \"down\", \"due\", \"during\",\n",
    "    \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\",\n",
    "    \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\",\n",
    "    \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\",\n",
    "    \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\",\n",
    "    \"further\", \"get\", \"give\", \"go\", \"got\", \"had\", \"has\", \"hasnt\",\n",
    "    \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\",\n",
    "    \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"into\", \"is\", \"it\", \"its\", \"it's\", \"itself\", \"just\", \"keep\", \"last\",\n",
    "    \"latter\", \"latterly\", \"least\", \"less\", \"like\", \"ltd\", \"made\", \"make\",\n",
    "    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n",
    "    \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n",
    "    \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\",\n",
    "    \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\",\n",
    "    \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\",\n",
    "    \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "    \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"people\", \"per\",\n",
    "    \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"said\", \"same\", \"see\",\n",
    "    \"seem\", \"seemed\", \"seeming\", \"seems\", \"several\", \"she\", \"should\",\n",
    "    \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "    \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\",\n",
    "    \"somewhere\", \"still\", \"such\", \"take\", \"ten\", \"than\", \"that\", \"the\",\n",
    "    \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\",\n",
    "    \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\",\n",
    "    \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\",\n",
    "    \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\",\n",
    "    \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\",\n",
    "    \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"use\", \"very\",\n",
    "    \"via\", \"want\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\",\n",
    "    \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\",\n",
    "    \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\",\n",
    "    \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "    \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\",\n",
    "    \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"reuters\", \"news\",\n",
    "    \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\",\n",
    "    \"sunday\", \"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\",\n",
    "    \"rappler\", \"rapplercom\", \"inquirer\", \"yahoo\", \"home\", \"sports\",\n",
    "    \"1\", \"10\", \"2012\", \"sa\", \"says\", \"tweet\", \"pm\", \"home\", \"homepage\",\n",
    "    \"sports\", \"section\", \"newsinfo\", \"stories\", \"story\", \"photo\",\n",
    "    \"2013\", \"na\", \"ng\", \"ang\", \"year\", \"years\", \"percent\", \"ko\", \"ako\",\n",
    "    \"yung\", \"yun\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"time\",\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\",\n",
    "    \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "    \"government\", \"police\"\n",
    "}\n",
    "ideal = 20.0\n",
    "\n",
    "\n",
    "def SummarizeUrl(url):\n",
    "    try:\n",
    "        article = grab_link(url)\n",
    "    except IOError:\n",
    "        print('IOError')\n",
    "        return None\n",
    "\n",
    "    if not (article and article.cleaned_text and article.title):\n",
    "        return None\n",
    "\n",
    "    return Summarize(article.title, article.cleaned_text)\n",
    "\n",
    "\n",
    "def Summarize(title, text):\n",
    "    sentences = split_sentences(text)\n",
    "    keys = keywords(text)\n",
    "    titleWords = split_words(title)\n",
    "\n",
    "    if len(sentences) <= 5:\n",
    "        return sentences\n",
    "\n",
    "    #score setences, and use the top 5 sentences\n",
    "    sentence_ranks = score(sentences, titleWords, keys)\n",
    "    return [sentence for sentence, score in sentence_ranks.most_common(5)]\n",
    "\n",
    "goose = Goose()\n",
    "def grab_link(url):\n",
    "    #extract article information using Python Goose\n",
    "    try:\n",
    "        article = goose.extract(url=url)\n",
    "        return article\n",
    "    except ValueError:\n",
    "        print('Goose failed to extract article from url')\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def score(sentences, titleWords, keywords):\n",
    "    #score sentences based on different features\n",
    "\n",
    "    senSize = len(sentences)\n",
    "    ranks = Counter()\n",
    "    for i, s in enumerate(sentences):\n",
    "        sentence = split_words(s)\n",
    "        titleFeature = title_score(titleWords, sentence)\n",
    "        sentenceLength = length_score(sentence)\n",
    "        sentencePosition = sentence_position(i+1, senSize)\n",
    "        sbsFeature = sbs(sentence, keywords)\n",
    "        dbsFeature = dbs(sentence, keywords)\n",
    "        frequency = (sbsFeature + dbsFeature) / 2.0 * 10.0\n",
    "\n",
    "        #weighted average of scores from four categories\n",
    "        totalScore = (titleFeature*1.5 + frequency*2.0 +\n",
    "                      sentenceLength*1.0 + sentencePosition*1.0) / 4.0\n",
    "        ranks[s] = totalScore\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def sbs(words, keywords):\n",
    "    score = 0.0\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            score += keywords[word]\n",
    "    return (1.0 / fabs(len(words)) * score)/10.0\n",
    "\n",
    "\n",
    "def dbs(words, keywords):\n",
    "    if (len(words) == 0):\n",
    "        return 0\n",
    "\n",
    "    summ = 0\n",
    "    first = []\n",
    "    second = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in keywords:\n",
    "            score = keywords[word]\n",
    "            if first == []:\n",
    "                first = [i, score]\n",
    "            else:\n",
    "                second = first\n",
    "                first = [i, score]\n",
    "                dif = first[0] - second[0]\n",
    "                summ += (first[1]*second[1]) / (dif ** 2)\n",
    "\n",
    "    # number of intersections\n",
    "    k = len(set(keywords.keys()).intersection(set(words))) + 1\n",
    "    return (1/(k*(k+1.0))*summ)\n",
    "\n",
    "\n",
    "def split_words(text):\n",
    "    #split a string into array of words\n",
    "    try:\n",
    "        text = re.sub(r'[^\\w ]', '', text)  # strip special chars\n",
    "        return [x.strip('.').lower() for x in text.split()]\n",
    "    except TypeError:\n",
    "        print(\"Error while splitting characters\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def keywords(text):\n",
    "    \"\"\"get the top 10 keywords and their frequency scores\n",
    "    ignores blacklisted words in stopWords,\n",
    "    counts the number of occurrences of each word\n",
    "    \"\"\"\n",
    "    text = split_words(text)\n",
    "    numWords = len(text)  # of words before removing blacklist words\n",
    "    freq = Counter(x for x in text if x not in stopWords)\n",
    "\n",
    "    minSize = min(10, len(freq))  # get first 10\n",
    "    keywords = {x: y for x, y in freq.most_common(minSize)}  # recreate a dict\n",
    "\n",
    "    for k in keywords:\n",
    "        articleScore = keywords[k]*1.0 / numWords\n",
    "        keywords[k] = articleScore * 1.5 + 1\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    '''\n",
    "    The regular expression matches all sentence ending punctuation and splits the string at those points.\n",
    "    At this point in the code, the list looks like this [\"Hello, world\", \"!\" ... ]. The punctuation and all quotation marks\n",
    "    are separated from the actual text. The first s_iter line turns each group of two items in the list into a tuple,\n",
    "    excluding the last item in the list (the last item in the list does not need to have this performed on it). Then,\n",
    "    the second s_iter line combines each tuple in the list into a single item and removes any whitespace at the beginning\n",
    "    of the line. Now, the s_iter list is formatted correctly but it is missing the last item of the sentences list. The\n",
    "    second to last line adds this item to the s_iter list and the last line returns the full list.\n",
    "    '''\n",
    "    \n",
    "    sentences = re.split('(?<![A-ZА-ЯЁ])([.!?]\"?)(?=\\s+\\\"?[A-ZА-ЯЁ])', text)\n",
    "    s_iter = list(zip(*[iter(sentences[:-1])] * 2))\n",
    "    s_iter = [''.join(map(str,y)).lstrip() for y in s_iter]\n",
    "    s_iter.append(sentences[-1])\n",
    "    return s_iter\n",
    "\n",
    "\n",
    "\n",
    "def length_score(sentence):\n",
    "    return 1 - fabs(ideal - len(sentence)) / ideal\n",
    "\n",
    "\n",
    "def title_score(title, sentence):\n",
    "    title = [x for x in title if x not in stopWords]\n",
    "    count = 0.0\n",
    "    for word in sentence:\n",
    "        if (word not in stopWords and word in title):\n",
    "            count += 1.0\n",
    "            \n",
    "    if len(title) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return count/len(title)\n",
    "\n",
    "\n",
    "def sentence_position(i, size):\n",
    "    \"\"\"different sentence positions indicate different\n",
    "    probability of being an important sentence\"\"\"\n",
    "\n",
    "    normalized = i*1.0 / size\n",
    "    if 0 < normalized <= 0.1:\n",
    "        return 0.17\n",
    "    elif 0.1 < normalized <= 0.2:\n",
    "        return 0.23\n",
    "    elif 0.2 < normalized <= 0.3:\n",
    "        return 0.14\n",
    "    elif 0.3 < normalized <= 0.4:\n",
    "        return 0.08\n",
    "    elif 0.4 < normalized <= 0.5:\n",
    "        return 0.05\n",
    "    elif 0.5 < normalized <= 0.6:\n",
    "        return 0.04\n",
    "    elif 0.6 < normalized <= 0.7:\n",
    "        return 0.06\n",
    "    elif 0.7 < normalized <= 0.8:\n",
    "        return 0.04\n",
    "    elif 0.8 < normalized <= 0.9:\n",
    "        return 0.04\n",
    "    elif 0.9 < normalized <= 1.0:\n",
    "        return 0.15\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T08:14:09.805485Z",
     "start_time": "2020-04-12T08:14:00.654381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. waiting for page to appear. wait for 1s...\n",
      "2. waiting for page to appear. wait for 1s...\n",
      "Covid wait_status = 1\n",
      "Article 1 : WHO Director-General's opening remarks at the media briefing on COVID-19 - 20 April 2020\n",
      "\n",
      "['Working together with the Global Fund, UNICEF and Unitaid, we have now placed orders for 30 million tests over the next four months.', 'The event raised more than US$127 million to support several organizations responding to COVID-19, including US$55 million for WHO’s Solidarity Response Fund.', 'We would like to make our UN truly UN, truly multilateral by including more languages and communicating with the whole world.', 'I appreciate the expressions of support from many countries for WHO’s coordinating role and our technical guidance.', 'Through April and May we intend to ship almost 180 million surgical masks, 54 million N95 masks and more than 3 million protective goggles to countries that need them most.']\n",
      "Working together with the Global Fund, UNICEF and Unitaid, we have now placed orders for 30 million tests over the next four months.\n",
      "\n",
      "The event raised more than US$127 million to support several organizations responding to COVID-19, including US$55 million for WHO’s Solidarity Response Fund.\n",
      "\n",
      "We would like to make our UN truly UN, truly multilateral by including more languages and communicating with the whole world.\n",
      "\n",
      "I appreciate the expressions of support from many countries for WHO’s coordinating role and our technical guidance.\n",
      "\n",
      "Through April and May we intend to ship almost 180 million surgical masks, 54 million N95 masks and more than 3 million protective goggles to countries that need them most.\n",
      "\n",
      "--------------------\n",
      "Article 2 : Maintaining routine immunization services vital during the COVID-19 pandemic\n",
      "\n",
      "['Joint WHO and UNICEF statement to mark European Immunization Week 2020\\n\\nThe COVID-19 pandemic is a stark reminder that infectious diseases know no borders.', 'Afshan Khan, UNICEF Regional Director for Europe and Central Asia. “It is critical that routine immunization programmes continue during this crisis, while adequately protecting health workers and individuals receiving vaccinations.', 'As scientists around the world work to develop a vaccine against the novel coronavirus and health care capacities are stretched in responding to COVID-19, national routine immunization programmes are more critical than ever before.', 'The urgent need for a COVID-19 vaccine underscores the pivotal role immunizations play in protecting lives and economies.', 'Reaching the most vulnerable children who have missed routine immunizations in the past should be prioritized.”\\n\\nIf, during these unprecedented times, local COVID-19 response measures cause temporary interruptions of routine immunization services, countries should plan to resume immunization services as quickly as possible after the situation stabilizes.']\n",
      "Joint WHO and UNICEF statement to mark European Immunization Week 2020\n",
      "\n",
      "The COVID-19 pandemic is a stark reminder that infectious diseases know no borders.\n",
      "\n",
      "Afshan Khan, UNICEF Regional Director for Europe and Central Asia. “It is critical that routine immunization programmes continue during this crisis, while adequately protecting health workers and individuals receiving vaccinations.\n",
      "\n",
      "As scientists around the world work to develop a vaccine against the novel coronavirus and health care capacities are stretched in responding to COVID-19, national routine immunization programmes are more critical than ever before.\n",
      "\n",
      "The urgent need for a COVID-19 vaccine underscores the pivotal role immunizations play in protecting lives and economies.\n",
      "\n",
      "Reaching the most vulnerable children who have missed routine immunizations in the past should be prioritized.”\n",
      "\n",
      "If, during these unprecedented times, local COVID-19 response measures cause temporary interruptions of routine immunization services, countries should plan to resume immunization services as quickly as possible after the situation stabilizes.\n",
      "\n",
      "--------------------\n",
      "Article 3 : WHO provides COVID-19 response supplies to Kurdistan Region of Iraq\n",
      "\n",
      "['Erbil, Iraq, 19 April 2020 – The World Health Organization (WHO) has provided medical supplies and equipment to the Ministry of Health of the Kurdistan region of Iraq to support response efforts in fighting COVID-19.', 'As of 18 April 2020, Kurdistan health authorities reported 337 confirmed cases of COVID-19 with 4 associated deaths and 244 recoveries.', 'The supplies will support COVID-19 response activities implemented by the directorates of health in Kurdistan.', \"This important shipment was welcomed by health officials in Kurdistan. “We appreciate WHO's support to the Ministry of Health.\", 'WHO gratefully acknowledges the support of Kuwait and USAID and their generous contribution which made possible the provision of these much needed medical supplies and equipment to the Ministry of Health of Kurdistan to fight COVID-19.']\n",
      "Erbil, Iraq, 19 April 2020 – The World Health Organization (WHO) has provided medical supplies and equipment to the Ministry of Health of the Kurdistan region of Iraq to support response efforts in fighting COVID-19.\n",
      "\n",
      "As of 18 April 2020, Kurdistan health authorities reported 337 confirmed cases of COVID-19 with 4 associated deaths and 244 recoveries.\n",
      "\n",
      "The supplies will support COVID-19 response activities implemented by the directorates of health in Kurdistan.\n",
      "\n",
      "This important shipment was welcomed by health officials in Kurdistan. “We appreciate WHO's support to the Ministry of Health.\n",
      "\n",
      "WHO gratefully acknowledges the support of Kuwait and USAID and their generous contribution which made possible the provision of these much needed medical supplies and equipment to the Ministry of Health of Kurdistan to fight COVID-19.\n",
      "\n",
      "--------------------\n",
      "Article 4 : Director-General’s Statement at G20 Health Ministers virtual meeting\n",
      "\n",
      "['We are encouraged that several G20 countries are now starting to plan how to ease social restrictions.', 'We are all facing an unprecedented global health crisis.', 'Second, we are looking to the G20 countries to continue to support the global response to COVID-19.', 'And we continue to fulfil our mandate to coordinate the global response, working with partners to save lives.', 'We are deeply concerned that the virus now appears to be gathering pace in countries that lack the capacity of many G20 countries to respond to it.']\n",
      "We are encouraged that several G20 countries are now starting to plan how to ease social restrictions.\n",
      "\n",
      "We are all facing an unprecedented global health crisis.\n",
      "\n",
      "Second, we are looking to the G20 countries to continue to support the global response to COVID-19.\n",
      "\n",
      "And we continue to fulfil our mandate to coordinate the global response, working with partners to save lives.\n",
      "\n",
      "We are deeply concerned that the virus now appears to be gathering pace in countries that lack the capacity of many G20 countries to respond to it.\n",
      "\n",
      "--------------------\n",
      "Article 5 : WHO Director-General's opening remarks at the media briefing on COVID-19 - 17 April 2020\n",
      "\n",
      "['Thank you so much, Lady Gaga and thank you Hugh.', 'For further details about tomorrow’s events, I’m delighted to welcome once again my friend and my brother Hugh Evans to say a few words, to be followed by the amazing Lady Gaga.', 'I share what Lady Gaga said: what the world needs is love and solidarity.', 'I would like to say thank you so much, thank you from our heart to those who have contributed.', 'So far, the Solidarity Response Fund has generated more than US$150 million from more than 245,000 individuals, corporations and foundations.']\n",
      "Thank you so much, Lady Gaga and thank you Hugh.\n",
      "\n",
      "For further details about tomorrow’s events, I’m delighted to welcome once again my friend and my brother Hugh Evans to say a few words, to be followed by the amazing Lady Gaga.\n",
      "\n",
      "I share what Lady Gaga said: what the world needs is love and solidarity.\n",
      "\n",
      "I would like to say thank you so much, thank you from our heart to those who have contributed.\n",
      "\n",
      "So far, the Solidarity Response Fund has generated more than US$150 million from more than 245,000 individuals, corporations and foundations.\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# go to the news website\n",
    "t.url('https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/news')\n",
    "\n",
    "wait_for_pageload('//p[@class=\"heading text-underline\"]')\n",
    "\n",
    "num_news = t.count('//p[@class=\"heading text-underline\"]') \n",
    "if num_news > 5:\n",
    "    num_news = 5\n",
    "\n",
    "for n in range(1, num_news+1):\n",
    "    news_link = t.read(f'(//p[@class=\"heading text-underline\"])[{n}]/ancestor-or-self::a/@href')\n",
    "    news_title = t.read(f'(//p[@class=\"heading text-underline\"])[{n}]/ancestor-or-self::a/@aria-label')\n",
    "    print('Article', n, \":\", news_title)\n",
    "    print('')\n",
    "    news_summaries = SummarizeUrl(news_link)\n",
    "    print(news_summaries)\n",
    "    for sentence in news_summaries:\n",
    "        print(sentence)\n",
    "        print('')\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
